{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52790b87",
   "metadata": {},
   "source": [
    "**1. What is the core assumption of Naive Bayes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dac1f9",
   "metadata": {},
   "source": [
    "- Naive Bayes assumes that all features (or input variables) are independent of each other given the class label. This means the presence or value of one feature does not affect any other feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc3adb",
   "metadata": {},
   "source": [
    "2. Differentiate between GaussianNB, MultinomialNB, and BernoulliNB:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e48377",
   "metadata": {},
   "source": [
    "- GaussianNB is used when features are continuous and follow a normal (Gaussian) distribution.\n",
    "- MultinomialNB is used for discrete counts, like word frequencies in text data.\n",
    "- BernoulliNB is used when features are binary (0 or 1), like whether a word exists in a document or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b682340",
   "metadata": {},
   "source": [
    "**3. Why is Naive Bayes considered suitable for high-dimensional data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e294e",
   "metadata": {},
   "source": [
    "- Naive Bayes is fast and simple because it makes the independence assumption, so it doesn't need to calculate complex relationships. It also works well even when the number of features is very large, like in text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71f1bb",
   "metadata": {},
   "source": [
    "Task 2: Spam Detection using MultinomialNB \n",
    "- ● Load a text dataset (e.g., SMS Spam Collection or any public text \n",
    "dataset). \n",
    "- ● Preprocess using CountVectorizer or TfidfVectorizer. \n",
    "- ● Train a MultinomialNB classifier. \n",
    "- ● Evaluate: \n",
    "- ○ Accuracy \n",
    "- ○ Precision \n",
    "- ○ Recall \n",
    "- ○ Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc33635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy-- 0.9668161434977578\n",
      "Precision- 1.0\n",
      "Recall 0.7516778523489933\n",
      "Confusion Matrix:\n",
      " [[966   0]\n",
      " [ 37 112]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\", sep='\\t', header=None, names=['label', 'message'])\n",
    "texts = data['message']\n",
    "labels = data['label'].map({'ham': 0, 'spam': 1})\n",
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(texts)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "model = MultinomialNB()\n",
    "model.fit(trainX, trainY)\n",
    "preds = model.predict(testX)\n",
    "print(\"Accuracy--\", accuracy_score(testY, preds))\n",
    "print(\"Precision-\", precision_score(testY, preds))\n",
    "print(\"Recall\", recall_score(testY, preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(testY, preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff6713",
   "metadata": {},
   "source": [
    "Task 3: GaussianNB with Iris or Wine Dataset \n",
    "- ● Train a GaussianNB classifier on a numeric dataset. \n",
    "- ● Split data into train/test sets. \n",
    "- ● Evaluate model performance. \n",
    "- ● Compare with Logistic Regression or Decision Tree briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "306daec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Accu: 0.9777777777777777\n",
      "Logistic Regression Accuracy:- 1.0\n",
      "Decision Tree Accu: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "targets = iris.target\n",
    "# Splitting the dataset into training and testing sets\n",
    "trainX, testX, trainY, testY = train_test_split(features, targets, test_size=0.3, random_state=42)\n",
    "nb = GaussianNB()\n",
    "nb.fit(trainX, trainY)\n",
    "predNB = nb.predict(testX)\n",
    "print(\"GaussianNB Accu:\", accuracy_score(testY, predNB))\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(trainX, trainY)\n",
    "predLR = lr.predict(testX)\n",
    "print(\"Logistic Regression Accuracy:-\", accuracy_score(testY, predLR))\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(trainX, trainY)\n",
    "predDT = dt.predict(testX)\n",
    "print(\"Decision Tree Accu:\", accuracy_score(testY, predDT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
